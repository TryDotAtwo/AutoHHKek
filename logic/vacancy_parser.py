import asyncio
from playwright.async_api import Page
from typing import List, Dict, Tuple
import re
import random
import json
import os

CACHE_FILE = 'vacancies_cache.json'

def load_cache() -> List[Dict[str, str]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ –∏–∑ JSON."""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
                print(f"–ó–∞–≥—Ä—É–∂–µ–Ω –∫—ç—à —Å {len(cache)} –≤–∞–∫–∞–Ω—Å–∏—è–º–∏.")
                return cache
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
    return []

def save_cache(vacancies: List[Dict[str, str]]):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –≤ JSON."""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(vacancies, f, ensure_ascii=False, indent=2)
        print(f"–ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π.")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")



async def get_total_vacancies(page: Page) -> int:
    try:
        all_text = await page.inner_text("body")
        match = re.search(r'–ù–∞–π–¥–µ–Ω–æ.*?(\d+).*?–ø–æ–¥—Ö–æ–¥—è—â–∏—Ö.*?–≤–∞–∫–∞–Ω—Å–∏[—è–π].*?–¥–ª—è.*?—Ä–µ–∑—é–º–µ', all_text, re.IGNORECASE | re.UNICODE | re.DOTALL)
        return int(match.group(1)) if match else 0
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞: {e}")
        return 0



async def get_search_session_id(page: Page) -> str:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ searchSessionId –∏–∑ URL."""
    try:
        current_url = page.url
        # print(f"DEBUG: current_url: {current_url}")
        match = re.search(r'searchSessionId=([a-f0-9-]+)', current_url)
        if match:
            return match.group(1)
        return ""
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ searchSessionId: {e}")
        return ""

async def get_max_pages_from_pagination(page: Page, total_count: int = 0) -> int:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏."""
    try:
        # –ò—â–µ–º –≤–Ω—É—Ç—Ä–∏ nav[data-qa="pager-block"]
        pager_container = await page.query_selector("nav[data-qa='pager-block']")
        if not pager_container:
            return 0
        
        page_links = await pager_container.query_selector_all("a[data-qa='pager-page']")
        max_page = 0
        for link in page_links:
            href = await link.get_attribute("href")
            match = re.search(r'page=(\d+)', href)
            if match:
                page_num = int(match.group(1))
                max_page = max(max_page, page_num)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–Ω–æ–ø–∫–∞ "–î–∞–ª–µ–µ", –¥–æ–±–∞–≤–ª—è–µ–º +1
        next_btn = await pager_container.query_selector("a[data-qa='pager-next']")
        if next_btn:
            max_page += 1
        
        # –ï—Å–ª–∏ total_count –∏–∑–≤–µ—Å—Ç–Ω–æ, –≤—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (100 –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
        if total_count > 0:
            estimated_pages = (total_count + 99) // 100
            max_page = max(max_page, estimated_pages)
        
        return max_page + 1  # +1, —Ç–∞–∫ –∫–∞–∫ page=0 ‚Äî –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return 0

def find_matching_sequence(cache: List[Dict[str, str]], current_page_vacancies: List[Dict[str, str]]) -> int:
    """–ù–∞—Ö–æ–¥–∏—Ç –∏–Ω–¥–µ–∫—Å –≤ –∫—ç—à–µ, –≥–¥–µ current_page_vacancies —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏–∑ 100 –≤–∞–∫–∞–Ω—Å–∏–π."""
    if len(current_page_vacancies) != 100:
        return -1  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–æ–≤–Ω–æ 100
    
    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: —Ç–æ–ª—å–∫–æ title (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º URL, —Ç.–∫. –æ–Ω–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ)
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, —Ç–∏—Ä–µ, —Å–ª—ç—à–∏; –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã; lower
    def norm_title(title):
        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        title_clean = re.sub(r'[.,!?;:\-/()]+', ' ', title.strip())  # –£–±–∏—Ä–∞–µ–º ., !? ; : - / ( )
        title_norm = re.sub(r'\s+', ' ', title_clean).lower().strip()
        return title_norm
    
    current_norm = [norm_title(vac['title']) for vac in current_page_vacancies]
    cache_norm = [norm_title(vac['title']) for vac in cache]
    
    # Debug: print –ø–µ—Ä–≤—ã—Ö 3 –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    print("üîç Debug: –ü–µ—Ä–≤—ã–µ 3 –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö title –∏–∑ current —Å—Ç—Ä–∞–Ω–∏—Ü—ã:")
    for i in range(min(3, len(current_norm))):
        print(f"  {i+1}: {current_norm[i]}")
    print("üîç –ü–µ—Ä–≤—ã–µ 3 –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö title –∏–∑ –∫—ç—à–∞:")
    for i in range(min(3, len(cache_norm))):
        print(f"  {i+1}: {cache_norm[i]}")
    
    for i in range(len(cache_norm) - 99):
        if cache_norm[i:i+100] == current_norm:
            print(f"–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –∫—ç—à–µ–º –Ω–∞—á–∏–Ω–∞—è —Å –∏–Ω–¥–µ–∫—Å–∞ {i} (100 –≤–∞–∫–∞–Ω—Å–∏–π –∏–¥–µ–Ω—Ç–∏—á–Ω—ã –ø–æ –ø–æ—Ä—è–¥–∫—É).")
            return i
    
    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π fuzzy fallback: –µ—Å–ª–∏ >90% —Å–æ–≤–ø–∞–¥–∞—é—Ç (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    # from difflib import SequenceMatcher
    # max_similarity = 0
    # best_i = -1
    # for i in range(len(cache_norm) - 99):
    #     sim = SequenceMatcher(None, cache_norm[i:i+100], current_norm).ratio()
    #     if sim > 0.9 and sim > max_similarity:
    #         max_similarity = sim
    #         best_i = i
    # if best_i != -1:
    #     print(f"Fuzzy —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (90%+) —Å –∏–Ω–¥–µ–∫—Å–∞ {best_i} (similarity: {max_similarity:.2f}).")
    #     return best_i
    
    print("–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–∂–µ –ø–æ—Å–ª–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.")
    return -1

async def search_vacancies(page: Page, resume_id: str, initial_max_pages: int = 100) -> Tuple[List[Dict[str, str]], int]:
    """–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ —Ä–µ–∑—é–º–µ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º max_pages –∏–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    print("–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –ø–æ —Ä–µ–∑—é–º–µ...")
    # –£–±–∏—Ä–∞–µ–º area=1 –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Å–µ–π –†–æ—Å—Å–∏–∏, –µ—Å–ª–∏ –≤ –ú–æ—Å–∫–≤–µ 0
    base_url = f"https://hh.ru/search/vacancy?resume={resume_id}&hhtmFromLabel=rec_vacancy_show_all&hhtmFrom=main&search_field=name&search_field=company_name&search_field=description&enable_snippets=true&forceFiltersSaving=true&items_on_page=100"
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    await page.goto(base_url, wait_until="domcontentloaded", timeout=60000)
    print("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —Ä–µ–∑—é–º–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
    
    # –°–¥–µ–ª–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç –¥–ª—è –¥–µ–±–∞–≥–∞
    await page.screenshot(path='debug_search_page.png')
    # print("DEBUG: –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–∫ debug_search_page.png")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ session_id –∏ total_count
    search_session_id = await get_search_session_id(page)
    if search_session_id:
        base_url += f"&searchSessionId={search_session_id}"
    total_count = await get_total_vacancies(page)
    print(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ hh.ru: {total_count}")
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (current_page_num=0)
    current_page_num = 0
    page_url = f"{base_url}&page={current_page_num}"
    await page.goto(page_url, wait_until="domcontentloaded", timeout=60000)
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ 1 (URL: page={current_page_num})")
    
    try:
        await page.wait_for_selector("div.vacancy-info--ieHKDTkezpEj0Gsx", timeout=30000)
    except:
        print("–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")
        return [], total_count
    
    vacancy_blocks = await page.query_selector_all("div.vacancy-info--ieHKDTkezpEj0Gsx")
    first_page_vacancies = []
    for block in vacancy_blocks:
        title_elem = await block.query_selector("a[data-qa='serp-item__title']")
        if title_elem:
            title = await title_elem.inner_text()
            href = await title_elem.get_attribute("href")
            if href and not href.startswith('https://'):
                href = f"https://hh.ru{href}" if href.startswith('/') else f"https://hh.ru/{href}"
            first_page_vacancies.append({"title": title, "url": href})
    
    if len(first_page_vacancies) != 100:
        print(f"–ù–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ –Ω–∞–π–¥–µ–Ω–æ {len(first_page_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π, –Ω–µ 100 ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –∑–∞–Ω–æ–≤–æ.")
        use_cache = False
        all_vacancies = first_page_vacancies
        start_page_num = 1
    else:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞: —Å–∫–∏–ø –µ—Å–ª–∏ total_count —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è
        cache = load_cache()
        if abs(total_count - len(cache)) / max(1, len(cache)) > 0.05:  # >5% —Ä–∞–∑–Ω–∏—Ü–∞ ‚Äî —Å–∫–∏–ø
            print(f"Total_count ({total_count}) —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –∫—ç—à–∞ ({len(cache)}), –æ–±–Ω–æ–≤–ª—è–µ–º –∑–∞–Ω–æ–≤–æ.")
            use_cache = False
            all_vacancies = first_page_vacancies
            start_page_num = 1
        else:
            matching_index = find_matching_sequence(cache, first_page_vacancies)
            if matching_index != -1:
                print(f"–ö—ç—à –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –∏–Ω–¥–µ–∫—Å–∞ {matching_index} –∏–∑ –∫—ç—à–∞.")
                all_vacancies = cache[matching_index:]
                use_cache = True
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ—Å–ª–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                start_page_num = current_page_num + 1
            else:
                print("–ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫—ç—à–µ–º ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –∑–∞–Ω–æ–≤–æ.")
                use_cache = False
                all_vacancies = first_page_vacancies
                start_page_num = 1
    print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ 1: –Ω–∞–π–¥–µ–Ω–æ {len(first_page_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π.")
    
    if not use_cache:
        # –ï—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à, –ø–∞—Ä—Å–∏–º –≤—Å–µ –∑–∞–Ω–æ–≤–æ
        current_page_num = start_page_num
        max_pages = initial_max_pages
        while current_page_num < max_pages:
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ max_pages –∏–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            dynamic_max = await get_max_pages_from_pagination(page, total_count)
            if dynamic_max > 0:
                max_pages = max(max_pages, dynamic_max)
                print(f"–û–±–Ω–æ–≤–ª—ë–Ω max_pages –∏–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {max_pages}")
            
            page_url = f"{base_url}&page={current_page_num}"
            await page.goto(page_url, wait_until="domcontentloaded", timeout=60000)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page_num + 1} (URL: page={current_page_num})")
            
            try:
                await page.wait_for_selector("div.vacancy-info--ieHKDTkezpEj0Gsx", timeout=30000)
            except:
                print(f"–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page_num + 1}. –í–æ–∑–º–æ–∂–Ω–æ, –∫–æ–Ω–µ—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.")
                break
            
            vacancy_blocks = await page.query_selector_all("div.vacancy-info--ieHKDTkezpEj0Gsx")
            page_vacancies = []
            for block in vacancy_blocks:
                title_elem = await block.query_selector("a[data-qa='serp-item__title']")
                if title_elem:
                    title = await title_elem.inner_text()
                    href = await title_elem.get_attribute("href")
                    if href and not href.startswith('https://'):
                        href = f"https://hh.ru{href}" if href.startswith('/') else f"https://hh.ru/{href}"
                    page_vacancies.append({"title": title, "url": href})
            
            if len(page_vacancies) == 0:
                print(f"–ù–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page_num + 1}. –ö–æ–Ω–µ—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.")
                break
            
            all_vacancies.extend(page_vacancies)
            print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page_num + 1}: –Ω–∞–π–¥–µ–Ω–æ {len(page_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_vacancies)} (–æ–±—â–µ–µ –Ω–∞ hh.ru: {total_count})")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ —á–∏—Å–ª–æ –æ—Ç–ø–∞—Ä—à–µ–Ω–Ω—ã—Ö —Å–æ–≤–ø–∞–ª–æ —Å –æ–±—â–∏–º - –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
            if len(all_vacancies) >= total_count:
                print(f"–ß–∏—Å–ª–æ –æ—Ç–ø–∞—Ä—à–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π ({len(all_vacancies)}) –¥–æ—Å—Ç–∏–≥–ª–æ –æ–±—â–µ–≥–æ ({total_count}) - –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥.")
                break
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            next_link = await page.query_selector(f"a[href*='page={current_page_num + 1}']")
            if not next_link:
                print("–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∫–æ–Ω–µ—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.")
                break
            
            current_page_num += 1
            await asyncio.sleep(random.uniform(2, 4))  # –°–ª—É—á–∞–π–Ω–∞—è –ø–∞—É–∑–∞ 2-4 —Å–µ–∫
        
        save_cache(all_vacancies)
    else:
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ—Å–ª–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        current_page_num = start_page_num
        max_pages = initial_max_pages
        while current_page_num < max_pages:
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ max_pages –∏–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            dynamic_max = await get_max_pages_from_pagination(page, total_count)
            if dynamic_max > 0:
                max_pages = max(max_pages, dynamic_max)
                print(f"–û–±–Ω–æ–≤–ª—ë–Ω max_pages –∏–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏: {max_pages}")
            
            page_url = f"{base_url}&page={current_page_num}"
            await page.goto(page_url, wait_until="domcontentloaded", timeout=60000)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page_num + 1} (URL: page={current_page_num})")
            
            try:
                await page.wait_for_selector("div.vacancy-info--ieHKDTkezpEj0Gsx", timeout=30000)
            except:
                print(f"–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –≤–∞–∫–∞–Ω—Å–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page_num + 1}. –í–æ–∑–º–æ–∂–Ω–æ, –∫–æ–Ω–µ—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.")
                break
            
            vacancy_blocks = await page.query_selector_all("div.vacancy-info--ieHKDTkezpEj0Gsx")
            page_vacancies = []
            for block in vacancy_blocks:
                title_elem = await block.query_selector("a[data-qa='serp-item__title']")
                if title_elem:
                    title = await title_elem.inner_text()
                    href = await title_elem.get_attribute("href")
                    if href and not href.startswith('https://'):
                        href = f"https://hh.ru{href}" if href.startswith('/') else f"https://hh.ru/{href}"
                    page_vacancies.append({"title": title, "url": href})
            
            if len(page_vacancies) == 0:
                print(f"–ù–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page_num + 1}. –ö–æ–Ω–µ—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.")
                break
            
            all_vacancies.extend(page_vacancies)
            print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {current_page_num + 1}: –Ω–∞–π–¥–µ–Ω–æ {len(page_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_vacancies)} (–æ–±—â–µ–µ –Ω–∞ hh.ru: {total_count})")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ —á–∏—Å–ª–æ –æ—Ç–ø–∞—Ä—à–µ–Ω–Ω—ã—Ö —Å–æ–≤–ø–∞–ª–æ —Å –æ–±—â–∏–º - –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
            if len(all_vacancies) >= total_count:
                print(f"–ß–∏—Å–ª–æ –æ—Ç–ø–∞—Ä—à–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π ({len(all_vacancies)}) –¥–æ—Å—Ç–∏–≥–ª–æ –æ–±—â–µ–≥–æ ({total_count}) - –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥.")
                break
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            next_link = await page.query_selector(f"a[href*='page={current_page_num + 1}']")
            if not next_link:
                print("–°—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∫–æ–Ω–µ—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏.")
                break
            
            current_page_num += 1
            await asyncio.sleep(random.uniform(2, 4))  # –°–ª—É—á–∞–π–Ω–∞—è –ø–∞—É–∑–∞ 2-4 —Å–µ–∫
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –Ω–æ–≤—ã–º–∏ –≤–∞–∫–∞–Ω—Å–∏—è–º–∏
        cache = load_cache()  # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º, —á—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω–µ—Ü
        cache.extend(all_vacancies[len(first_page_vacancies):])  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ (–ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
        save_cache(cache)
        all_vacancies = cache[matching_index:]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å –ø–æ–∑–∏—Ü–∏–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    
    print(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(all_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∏–∑ {total_count} –Ω–∞ hh.ru).")
    if len(all_vacancies) == 0:
        print("–í–æ–∑–º–æ–∂–Ω–æ, —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –∏–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ª–æ–≥–∏–Ω.")
    return all_vacancies, total_count
